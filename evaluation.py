# evaluation.py
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm


def _calculate_precision_at_k(ranking, ground_truth, k):
    """(å†…éƒ¨é–¢æ•°) 1ã¤ã®ã‚¯ã‚¨ãƒªã®P@kã‚’è¨ˆç®—"""
    top_k = ranking[:k]
    ground_truth_set = set(ground_truth)

    hits = 0
    for idx in top_k:
        if idx in ground_truth_set:
            hits += 1

    return hits / k


def _calculate_ap(ranking, ground_truth):
    """(å†…éƒ¨é–¢æ•°) 1ã¤ã®ã‚¯ã‚¨ãƒªã®AP (Average Precision) ã‚’è¨ˆç®—"""
    ground_truth_set = set(ground_truth)
    num_ground_truth = len(ground_truth_set)

    if num_ground_truth == 0:
        return 0.0

    hits = 0
    sum_precision = 0

    for i, idx in enumerate(ranking):
        if idx in ground_truth_set:
            hits += 1
            precision_at_i = hits / (i + 1)
            sum_precision += precision_at_i

    return sum_precision / num_ground_truth


def run_evaluation(model, all_images, evaluation_sets, k_list=[5, 10, 20]):
    """
    ãƒ¢ãƒ‡ãƒ«ã®æ¤œç´¢æ€§èƒ½ã‚’è©•ä¾¡ã—ã€mAPã¨P@kã‚’è¨ˆç®—ãƒ»è¡¨ç¤ºã™ã‚‹ã€‚

    Args:
        model: å­¦ç¿’æ¸ˆã¿VAEãƒ¢ãƒ‡ãƒ«
        all_images (np.array): å…¨ç”»åƒãƒ‡ãƒ¼ã‚¿ (N, H, W, C)
        evaluation_sets (list): è©•ä¾¡ã‚»ãƒƒãƒˆã®ãƒªã‚¹ãƒˆ
        k_list (list): P@kã‚’è¨ˆç®—ã™ã‚‹kã®å€¤ã®ãƒªã‚¹ãƒˆ
    """

    print("\n--- ğŸ“ˆ è©•ä¾¡ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ã—ã¾ã™ ---")

    # --- 1. å…¨ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ï¼ˆEmbeddingï¼‰ã‚’æŠ½å‡º ---
    # VAEã®z_meanã‚’ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã®ãŒä¸€èˆ¬çš„
    print("å…¨ç”»åƒã‹ã‚‰ç‰¹å¾´é‡ï¼ˆz_meanï¼‰ã‚’æŠ½å‡ºä¸­...")
    # model.encoder ãŒ (z_mean, z_log_var, z) ã‚’è¿”ã™ã¨ä»®å®š
    z_mean, _, _ = model.encoder(all_images, training=False)
    all_embeddings = z_mean.numpy()  # (N, latent_dim)
    print(f"ç‰¹å¾´é‡æŠ½å‡ºå®Œäº†ã€‚å½¢çŠ¶: {all_embeddings.shape}")

    # --- 2. é¡ä¼¼åº¦è¡Œåˆ—ã®è¨ˆç®— ---
    print("å…¨ç”»åƒé–“ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—ä¸­...")
    # (N, N) ã®è¡Œåˆ—
    similarity_matrix = cosine_similarity(all_embeddings)
    print("é¡ä¼¼åº¦è¡Œåˆ—ã®è¨ˆç®—å®Œäº†ã€‚")

    # --- 3. å…¨ã‚¯ã‚¨ãƒªã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ä½œæˆ ---
    all_rankings = []

    for test_set in tqdm(evaluation_sets, desc="å…¨ã‚¯ã‚¨ãƒªã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ä½œæˆä¸­"):
        query_index = test_set["query_index"]

        # é¡ä¼¼åº¦ã‚’å–å¾—
        query_similarities = similarity_matrix[query_index]

        # é¡ä¼¼åº¦ãŒé«˜ã„é †ï¼ˆé™é †ï¼‰ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚½ãƒ¼ãƒˆ
        sorted_indices = np.argsort(query_similarities)[::-1]

        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‹ã‚‰è‡ªåˆ†è‡ªèº«ï¼ˆã‚¯ã‚¨ãƒªï¼‰ã‚’é™¤å¤–
        ranking = sorted_indices[1:]

        all_rankings.append(
            {
                "query_index": query_index,
                "ground_truth": test_set["ground_truth_indices"],
                "model_ranking": ranking,
            }
        )

    # --- 4. è©•ä¾¡æŒ‡æ¨™ï¼ˆmAP, P@kï¼‰ã®è¨ˆç®— ---

    # mAP
    total_ap = 0
    for item in all_rankings:
        total_ap += _calculate_ap(item["model_ranking"], item["ground_truth"])

    mAP = total_ap / len(all_rankings)

    # P@k
    p_at_k_results = {}
    for k in k_list:
        total_precision = 0
        for item in all_rankings:
            total_precision += _calculate_precision_at_k(
                item["model_ranking"], item["ground_truth"], k
            )

        mean_p_at_k = total_precision / len(all_rankings)
        p_at_k_results[k] = mean_p_at_k

    # --- 5. çµæœã®è¡¨ç¤º ---
    print("\n--- ğŸ“Š è©•ä¾¡çµæœ ---")
    print(f"è©•ä¾¡ã‚¯ã‚¨ãƒªæ•°: {len(all_rankings)}")
    print(f"mAP (Mean Average Precision): {mAP:.4f}")
    for k, value in p_at_k_results.items():
        print(f"Mean Precision@{k} (P@{k}): {value:.4f}")
    print("--------------------")
